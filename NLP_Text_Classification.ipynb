{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6ebfee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c39ce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset\n",
    "data = keras.datasets.imdb\n",
    "#Splitting the dataset\n",
    "(train_data,train_labels),(test_data,test_labels) = data.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26004a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The encoded data is:  [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "#View words(encoded)\n",
    "print(\"The encoded data is: \", train_data[0]) \n",
    "print(\"----------------------------\")\n",
    "data2 = test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "147f9ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping \n",
    "word_index = data.get_word_index() #this creates a dictionary with each key as word and each value as indices\n",
    "word_index = {k:(v+3) for k,v in word_index.items()} # Here 3 is added to each value of the key,\n",
    "#This is often done to leave room for special tokens (like padding, start-of-sequence, end-of-sequence)\n",
    "word_index['<PAD>'] = 0\n",
    "word_index['<START>'] = 1\n",
    "word_index['<UNK>'] = 2\n",
    "word_index['<UNUSED>'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6f520f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The decoded data is: <START> please give this one a miss br br <UNK> <UNK> and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite <UNK> so all you madison fans give this a miss\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([(value,key) for (key,value) in word_index.items()])#this returns a dictionary where each key is a index and each value is word\n",
    "def decode(text):\n",
    "    return \" \".join([reverse_word_index.get(i,'?') for i in text])\n",
    "print(f\"The decoded data is: {decode(data2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bda56cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of training data at index 0 is: 218 and at index 1 is: 189\n",
      "The length of training data and testing data after padding 250 250\n"
     ]
    }
   ],
   "source": [
    "#The length of the data\n",
    "print(f\"The length of training data at index 0 is: {len(train_data[0])} and at index 1 is: {len(train_data[1])}\") \n",
    "#Here the sentences has varying lengths,neural networks require input data to have specific shape each time data is fed \n",
    "#To overcome this issue we use padding\n",
    "#Preprocessing the data\n",
    "train_data = pad_sequences(train_data,value=word_index['<PAD>'],maxlen=250,padding=\"post\")\n",
    "test_data = pad_sequences(test_data,value=word_index['<PAD>'],maxlen=250,padding=\"post\")\n",
    "print(\"The length of training data and testing data after padding\",len(train_data[0]),len(test_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca39982a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 16)          160000    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 16)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                272       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160289 (626.13 KB)\n",
      "Trainable params: 160289 (626.13 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Embedding(10000,16))\n",
    "model.add(layers.GlobalAveragePooling1D())\n",
    "model.add(layers.Dense(16,activation='relu'))\n",
    "model.add(layers.Dense(1,activation= 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b6bd84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11f917c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "28/28 [==============================] - 2s 37ms/step - loss: 0.6916 - accuracy: 0.5748 - val_loss: 0.6892 - val_accuracy: 0.5875\n",
      "Epoch 2/20\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 0.6852 - accuracy: 0.6540 - val_loss: 0.6808 - val_accuracy: 0.7253\n",
      "Epoch 3/20\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 0.6729 - accuracy: 0.7054 - val_loss: 0.6659 - val_accuracy: 0.7492\n",
      "Epoch 4/20\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 0.6523 - accuracy: 0.7715 - val_loss: 0.6423 - val_accuracy: 0.7483\n",
      "Epoch 5/20\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 0.6223 - accuracy: 0.7838 - val_loss: 0.6109 - val_accuracy: 0.7576\n",
      "Epoch 6/20\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 0.5842 - accuracy: 0.8051 - val_loss: 0.5732 - val_accuracy: 0.8009\n",
      "Epoch 7/20\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 0.5409 - accuracy: 0.8259 - val_loss: 0.5335 - val_accuracy: 0.8176\n",
      "Epoch 8/20\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 0.4971 - accuracy: 0.8442 - val_loss: 0.4938 - val_accuracy: 0.8274\n",
      "Epoch 9/20\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 0.4548 - accuracy: 0.8592 - val_loss: 0.4590 - val_accuracy: 0.8397\n",
      "Epoch 10/20\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 0.4171 - accuracy: 0.8700 - val_loss: 0.4277 - val_accuracy: 0.8498\n",
      "Epoch 11/20\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 0.3840 - accuracy: 0.8770 - val_loss: 0.4015 - val_accuracy: 0.8562\n",
      "Epoch 12/20\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 0.3553 - accuracy: 0.8861 - val_loss: 0.3800 - val_accuracy: 0.8609\n",
      "Epoch 13/20\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 0.3306 - accuracy: 0.8921 - val_loss: 0.3624 - val_accuracy: 0.8636\n",
      "Epoch 14/20\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 0.3095 - accuracy: 0.8991 - val_loss: 0.3478 - val_accuracy: 0.8704\n",
      "Epoch 15/20\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 0.2915 - accuracy: 0.9027 - val_loss: 0.3357 - val_accuracy: 0.8726\n",
      "Epoch 16/20\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 0.2745 - accuracy: 0.9067 - val_loss: 0.3256 - val_accuracy: 0.8751\n",
      "Epoch 17/20\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 0.2604 - accuracy: 0.9120 - val_loss: 0.3176 - val_accuracy: 0.8762\n",
      "Epoch 18/20\n",
      "28/28 [==============================] - 1s 27ms/step - loss: 0.2469 - accuracy: 0.9159 - val_loss: 0.3109 - val_accuracy: 0.8769\n",
      "Epoch 19/20\n",
      "28/28 [==============================] - 1s 29ms/step - loss: 0.2352 - accuracy: 0.9194 - val_loss: 0.3045 - val_accuracy: 0.8806\n",
      "Epoch 20/20\n",
      "28/28 [==============================] - 1s 28ms/step - loss: 0.2238 - accuracy: 0.9239 - val_loss: 0.2998 - val_accuracy: 0.8813\n"
     ]
    }
   ],
   "source": [
    "#Splitting data into validation data and training data to prevent overfitting\n",
    "x_val =train_data[:10000] \n",
    "y_val = train_labels[:10000]\n",
    "x_train = train_data[10000:]\n",
    "y_train = train_labels[10000:]\n",
    "#Fitting the model\n",
    "modelf = model.fit(x_train,y_train,epochs= 20,batch_size=550,validation_data=(x_val,y_val),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85ac026a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 3ms/step - loss: 0.3145 - accuracy: 0.8730\n",
      "The loss is: 0.31449630856513977,and the accuracy is: 0.8730000257492065\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "test_loss, test_accuracy = model.evaluate(test_data,test_labels)\n",
    "print(f\"The loss is: {test_loss},and the accuracy is: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fb6cda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 137ms/step\n",
      "Review:\n",
      "focuses on mood and character development the plot is very simple and many of the scenes take place on the same set in frances <UNK> the sandy dennis character apartment but the film builds to a disturbing climax br br the characters create an atmosphere <UNK> with sexual tension and psychological <UNK> it's very interesting that robert altman directed this considering the style and structure of his other films still the trademark altman audio style is evident here and there i think what really makes this film work is the brilliant performance by sandy dennis it's definitely one of her darker characters but she plays it so perfectly and convincingly that it's scary michael burns does a good job as the mute young man regular altman player michael murphy has a small part the <UNK> moody set fits the content of the story very well in short this movie is a powerful study of loneliness sexual <UNK> and desperation be patient <UNK> up the atmosphere and pay attention to the wonderfully written script br br i praise robert altman this is one of his many films that deals with unconventional fascinating subject matter this film is disturbing but it's sincere and it's sure to <UNK> a strong emotional response from the viewer if you want to see an unusual film some might even say bizarre this is worth the time br br unfortunately it's very difficult to find in video stores you may have to buy it off the internet\n",
      "Prediction: [0.99658155]\n",
      "Actual: 1\n"
     ]
    }
   ],
   "source": [
    "#Making a prediction\n",
    "test_review = test_data[1]\n",
    "predict = model.predict(np.array([test_review])) \n",
    "print(\"Review:\")\n",
    "print(decode(test_review))\n",
    "print(\"Prediction: \" + str(predict[0])) #The str(predict[0]) part converts this numerical prediction into a string, \n",
    "#so it can be concatenated with the \"Prediction: \" string for display using the + operator\n",
    "print(\"Actual: \" + str(test_labels[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217ff884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
